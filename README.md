Post-Zephir Metadata Processing
===============================

A mostly haphazard collection of scripts (Bash, Perl) that take Zephir records, do some clean up, generate the Hathifiles, and calculate Bib Rights, among other processes.

Parts of these should likely be extracted into their own repositories, or obviated by a re-architecture. 

run_process_zephir_incremental.sh (daily)
=========================================
* Process daily file of new/updated/deleted metadata provided by Zephir
* Send deleted bib record IDs (provided by Zephir) to Bill
* "Clean up" zephir records
* (re)determine bibliographic rights
  + Write new/updated bib rights to file for Aaron's process to pick up and update the rights db (Why: possibly because of limited permissions on the rights database)
* File of processed new/updated records is copied to an HT server for Bill to index in the catalog
* Generate OAI update file and make available to the OAI update process (Roger, zephir2oai.pl)
  + Output written to nfs directory on server where OAI update process occurs 
* Delete old OAI files.
* Retrieve files with changed & new records daily_touched_YYYY-MM-DD.tsv.gz and groove_incremental_YYYY_MM-DD.tsv.gz from Zephir for pickup by ingest to be loaded to the `feed_zephir_items` table, which supports determining what items are newly available for ingest, what digitization source we expect to see for those items, and what their collection code (which maps to content provider and responsible source) is
* Retrieves full bib metadata file from zephir and runs run_zephir_full_daily.sh. (Why?)

Why?
----
The new/updated/deleted metadata provided by Zephir needs to make it to the catalog, and eventually into the rights database. 

Data In
-------
* `ht_bib_export_incr_YYYY-MM-DD.json.gz` (incremental updates from Zephir, `ftps_zephir_get`)
* `vufind_removed_cids_YYYY-MM-DD.txt.gz` (CIDs that have gone away, `ftps_zephir_get`)
* `groove_incremental_YYYY-MM-DD.tsv.gz`  (from Zephir - new items added to Zephir?)
* `/tmp/rights_dbm`  (taken from `ht_rights.rights_current` table in the rights database)
* `us_cities.db` (dependency for `bib_rights.pm`)
* `us_fed_pub_exception_file` (dependency for `bib_rights.pm`, `/htdata/govdocs/feddocs_oclc_filter/`) 

Data Out
--------
* `debug_current.txt` (what and why for this?)
* `zephir_upd_YYYYMMDD.rights` - picked up hourly by https://github.com/hathitrust/feed_jobs/blob/master/feed.hourly/populate_rights_data.pl and loaded into the `rights_current` table. Will be placed directly in /htapps/babel/feed/var/rights and will remove the scp logic from populate_rights_data.pl
* `zephir_upd_YYYYMMDD_delete.txt.gz`  will be moved to /htsolr/catalog/prep. Used by the catalog to process deletes.
* `zephir_upd_YYYYMMDD_dollar_dup.txt `(generated by post_zephir_cleanup.pl, gets sent to Zephir, ftps_zephir_send, Zephir uningests these duplicate records)
* `zephir_upd_YYYYMMDD.json.gz` will be sent to /htsolr/catalog/prep for [catalog indexing](https://github.com/hathitrust/hathitrust_catalog_indexer)
* Updated bibliographic records - used by https://github.com/hathitrust/feed_jobs/blob/master/feed.daily/02_get_bibrecords.pl to update the feed_zephir_items table on a daily basis. Could place directly in /htapps/babel/feed/var/bibrecords and remove the scp logic in `02_get_bibrecords.pl`, or just have `02_get_bibrecords.pl` call `ftps_zephir_get` directly: `daily_touched_YYYY-MM-DD.txt.gz` and `groove_incremental_YYYY-MM-DD.tsv.gz` (Retrieved with `ftps_zephir_get`.)  `daily_touched*.txt.gz` and `groove_incremental*.tsv.gz` will be placed in /htapps/babel/feed/var/bibrecords
* `zephir_oai_upd_YYYYMMDD_oaimarc_seqnum.xml` currently going to `/aleph-22_1/aleph/uprod/miu50/local/mdp_batch/zephir/oai_data`. We will instead put at /htdev/htdata/oai. Soon to be deprecated (2022-06-20).
* `meta_(ic|pd_google|pd_open_access|restricted)_20200107.jsonl.gz` (currently going to `/aleph-22_1/aleph/uprod/miu50/local/mdp_batch/return/transfer`) Used by datasets. Currently copied from gimlet to `/htprep/datasets/ht_bib` at ICTC by `/l/local/bin/getmeta.sh` on quik-1. New process will put into `/htprep/metadata` Soon to be deprecated (2022-06-20). 
* `zephir_ingested_items.txt.gz` - scped from `gimlet` to `/htapps/babel/feed/var/bibrecords` to temporary location `/ram/zephir_items` on macc-ht-ingest-001 by https://github.com/hathitrust/feed_jobs/blob/master/feed.monthly/zephir_diff.pl. Used to refresh the full `feed_zephir_items` table on a monthly basis. Will be deposited directly to /htapps/babel/feed/var/bibrecords
* `zephir_full_daily_rpt.txt` Does anyone need this?

Perl script dependencies
------------------------
* `bld_rights_db.pl` (builds `/tmp/rights_dbm`)
* `bib_rights.pm`
* `post_zephir_cleanup.pl`

Bash script dependencies
------------------------
* `ftps_zephir_get`
* `ftps_zephir_send`
* `run_process_zephir_full.sh`

run_zephir_full_daily.sh (daily, and monthly)
=============================================
* Pulls a full bib metadata file from zephir
* Assembles zephir_ingested_items.txt.gz and moves to /htapps/babel/feed/var/bibrecords
* ~~generates the HTRC datasets metadata (parameter to post_zephir_cleanup.pl)~~ deprecated (2022-08-24)
  + ~~Files written to a directory and rsynced to HTRC server (A&E process)~~ deprecated (2022-08-24)
* On the first of the month, processes the full zephir file:
  + Splits input file and runs multiple invocations of post_zephri_cleanup.pl in parallel
  + Generate new/updated bib rights

Why?
----
When run daily, mostly for HTRC purposes? 

Data In
-------
* US Fed Doc exception list `/htdata/govdocs/feddocs_oclc_filter/oclcs_removed_from_registry_YYYY-MM-DD.txt`
* `/tmp/rights_dbm`
* `groove_export_YYYY-MM-DD.tsv.gz` (ftps from cdlib)
* `ht_bib_export_full_YYYY-MM-DD.json.gz`

Data Out
--------
* `groove_export_YYYY-MM-DD.tsv.gz` will be moved to /htapps/babel/feed/var/bibrecords/groove_full.tsv.gz  
* ~~`meta_(ic, pd_google, pd_open_Access, restricted)_YYYYMMDD.jsonl.gz` put at /htprep/metadata~~
* `zephir_full_${YESTERDAY}_vufind.json.gz`, copied to /htdata/govdocs/zephir, and the zephir archive. Indexed into catalog via the same process as for `run_process_zephir_incremental.sh`
* `zephir_full_${YESTERDAY}.rights`, will be moved to /htapps/babel/feed/var/rights/
* `zephir_full_${YESTERDAY}.rights.debug`, doesn't appear to be used
* `zephir_full_daily_rpt.txt`

Perl script dependencies
------------------------
* `bld_rights_db.pl`
* `bib_rights.pm`
* `post_zephir_cleanup.pl`

Bash script dependencies
------------------------
* `ftps_zephir_get`
* `ftps_zephir_send`


Production Setup
=====
Fill out the config/.env and config/.netrc files. Replace the contents of config/config.pl with correct credentials.

```bash
wget https://cpan.metacpan.org/authors/id/H/HA/HAARG/local-lib-2.000024.tar.gz
tar -xzf local-lib-2.000024.tar.gz
cd local-lib-2.000024
perl Makefile.PL --bootstrap=/l1/govdocs/zcode/local
make test && make install
curl -L http://cpanmin.us | perl - App::cpanminus
cpanm --install-deps .
```

Running Tests
====
Tests with limited coverage can be run with Docker.

```bash
docker-compose build
docker-compose up -d
docker-compose run --rm pz perl t/test_postZephir.pl
```

For test coverage, replace the previous `docker-compose run` with
```bash
docker-compose run --rm pz bash -c "perl -MDevel::Cover=-silent,1 t/test_postZephir.pl && cover -nosummary /usr/src/app/cover_db"
```
