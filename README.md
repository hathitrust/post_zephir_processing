Post-Zephir Metadata Processing
===============================

A mostly haphazard collection of scripts (Bash, Perl) that take Zephir records, do some clean up, generate the Hathifiles, and calculate Bib Rights, among other processes.

Parts of these should likely be extracted into their own repositories, or obviated by a re-architecture. 

#todo: needs to be replaced in multiple files


run_hathi_volumes_ingested_zephir.sh (daily)
====================================
* Send file of ingested volumes to Zephir
* Concatenates all files of the proper name to one file to send
* File(s) provided by Aaron's processes
* Gets moved to root_dir/data/barcode_archive 

Why?
---
This appears to take the barcodes from Aaron (GRIN?) and sends them to Zephir. Why does Zephir need the barcodes?

Data In
-------
* barcodes_<year>-<mm>-<dd>_* (previously: /exlibris/aleph/uprod/miu50/local/mdp/return/

Data Out
--------
* hathi_volumes_ingested_<YYYYMMDD>.txt sent to ftps.cdlib.org (untested, needs .netrc)
* run_hathi_volumes_ingested_zephir.sh_<YYYYMMDD>_rpt.txt sent to $EMAIL (jstever@umich.edu)
* moving barcode input files into an archive directory

Perl script dependencies
------------------------
* None

Bash script dependencies
------------------------
* ftps_zephir_send ( shell script, dependent upon .netrc credentials)


get_all_repository_ids.sh (weekly)
=========================
* Get file of all HTIDs in the repository from the feed_audit table(?) and send to zephir
* Performs a single query on the mysql database. 
* Runs weekly. 

Why?
----
Auditing? Not clear why Zephir needs this. 

Data In
-------
* feed_audit table in ht_repository database 

Data Out
--------
* repository_ids.txt.gz get sent to Zephir (or they would, but I can't run that) 

Perl script dependencies
------------------------
* None

Bash script dependencies
------------------------
* ftps_zephir_send ( shell script, dependent upon .netrc credentials)


run_process_zephir_incremental.sh (daily)
=========================================
* Process daily file of new/updated/deleted metadata provided by Zephir
* Send deleted bib record IDs (provided by Zephir) to Bill
* "Clean up" zephir records
* (re)determine bibliographic rights
  + Write new/updated bib rights to file for Aaron's process to pick up and update the rights db (Why?)
* File of processed new/updated records is copied to an HT server for Bill to index in the catalog
* Generate daily hathifile update file
  + Send to HT file server used by the HT web server
  + SSH in and run some additional perl script to update stuff.
* Generate OAI update file and make available to the OAI update process (Jose, zephir2oai.pl)
  + Output written to nfs directory on server where OAI update process occurs
* Delete old OAI files.
* Retrieve daily_touched_<YYYY-MM-DD>.tsv.gz and move it somewhere for some unknown reason (Why?)
* Retrieve groove_incremental_<YYYY_MM-DD>.tsv.gz from Zephir, which is new metadata added to zephir (HTIDS)?, make available to "Aaron's processes". (Why?)
* Retrieves full bib metadata file from zephir and generates the HTRC datasets metadata with run_zephir_full_daily.sh. (Why?)

Why?
----
The new/updated/deleted metadata provided by Zephir needs to make it to the catalog, and eventually into the rights database. 

Data In
-------
* ht_bib_export_incr_<YYYY>-<MM>-<DD>.json.gz (incremental updates from Zephir)
* vufind_removed_cids_<YYYY>-<MM>-<DD>.txt.gz (CIDs that have gone away)
* groove_incremental_<YYYY-MM-DD>.tsv.gz  (from Zephir, candidates for ingest?)
* /tmp/rights_dbm 
* us_cities.db (dependency for bib_rights.pm)
* us_fed_pub_exception_file (dependency for bib_rights.pm)
* namespacemap.yml (namespaces, how is this maintained?)

Data Out
--------
* debug_current.txt (what and why for this?)
* zephir_upd_<YYYYMMDD>.rights
* zephir_upd_<YYYYMMDD>_delete.txt.gz (scp to solr/catalog/prep on the server, .ssh id required)
* hathi_upd_<YYYYMMDD>.txt (hathifile, scp to the HT web host)
* zephir_upd_<YYYYMMDD>_dollar_dup.txt (generated by zephir_hathifile.pl, gets sent to Zephir)
* daily_touched_<YYYY-MM-DD>.txt.gz
* groove_incremental_<YYYY-MM-DD>.tsv.gz ( 

Perl script dependencies
------------------------
* bld_rights_db.pl (builds /tmp/rights_dbm)
* bib_rights.pm 
* zephir_hathifile.pl 

Bash script dependencies
------------------------
* ftps_zephir_get
* ftps_zephir_send
* run_process_zephir_full.sh

run_zephir_full_daily.sh (daily, and monthly)
=============================================
* Pulls a full bib metadata file from zephir and generates the HTRC datasets metadata (parameter to zephir_hathifile.pl)
  + Files written to a directory and rsynced to HTRC server (A&E process)
* On the first of the month, processes the full zephir file:
  + Splits input file and runs multiple invocations of zephir_hathifiles.pl in parallel
  + Generate new/updated bib rights
  + Output a full hathifile, write to HT web fileserver
  + Remove oldest full hathifile and previous month's daily hathifiles

Why?
----
When run daily, mostly for HTRC purposes? 
When run on the 1st of the month, it takes care of the full hathifile. that goes on the website.

Data In
-------
* US Fed Doc exception list oclcs_removed_from_registry_<date>.txt
* /tmp/rights_dbm
* groove_export_<zephir_date>.tsv.gz (ftps from cdlib)
* ht_bib_export_full_<zephir_date>.json.gz

Data Out
--------
* groove_export_<zephir_date>.tsv.gz gets moved to return/groove_full.tsv.gz (why?)
* hathi_full_<date>.txt
* meta_<ic, pd_google, pd_open_Access, restricted>_<today>.jsonl.gz
* zephir_full_${YESTERDAY}_vufind.json.gz
* zephir_full_${YESTERDAY}.rights
* zephir_full_${YESTERDAY}.rights.tsv
* zephir_full_${YESTERDAY}.rights.debug
* zephir_full_daily_rpt.txt

Perl script dependencies
------------------------
* bld_rights_db.pl
* bib_rights.pm
* zephir_hathifile.pl

Bash script dependencies
------------------------
* ftps_zephir_get
* ftps_zephir_send


